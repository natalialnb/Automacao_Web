{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/ilVTwYfkm6Asr/M17bew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natalialnb/Automacao_Web/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"#cd7f32\"> **Mini Case PySpark** </font>\n",
        "**Jump Start**"
      ],
      "metadata": {
        "id": "zP98WoFm9T1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O Que é Particionamento?\n",
        "\n",
        "Particionamento é a forma como o PySpark divide um grande conjunto de dados em partes menores, chamadas de \"partições\", para processá-las de forma distribuída. Cada partição é um pedaço de dados que pode ser processado independentemente em paralelo. A ideia é distribuir a carga de trabalho entre diferentes nós de um cluster, para acelerar o processamento e tornar a análise de big data mais eficiente.\n",
        "\n",
        "### Como o Particionamento Funciona no PySpark\n",
        "\n",
        "1. **Partições de RDD**: No PySpark, os dados são representados por RDDs (Resilient Distributed Datasets). Um RDD é dividido em várias partições, que são processadas em paralelo. O número de partições pode ser ajustado para otimizar a performance.\n",
        "\n",
        "2. **DataFrames e Datasets**: Para DataFrames e Datasets, o particionamento é um pouco mais abstrato, mas o princípio é o mesmo. Dados são divididos em partições para serem processados em paralelo.\n",
        "\n",
        "3. **Paralelismo e Distribuição**: As partições são distribuídas entre os diferentes nós do cluster, permitindo que múltiplas operações sejam realizadas em paralelo. Isso é essencial para escalar o processamento de dados.\n",
        "\n",
        "### Como Configurar o Particionamento\n",
        "\n",
        "1. **Número de Partições**: Você pode definir o número de partições ao criar um RDD, usar a função `repartition()` ou `coalesce()` em DataFrames. O `repartition()` aumenta ou diminui o número de partições e pode causar um shuffle (redistribuição dos dados), enquanto o `coalesce()` é mais eficiente para reduzir o número de partições sem shuffle.\n",
        "\n",
        "   ```python\n",
        "   # Exemplo de reparticionamento\n",
        "   df = df.repartition(10)\n",
        "   ```\n",
        "\n",
        "2. **Coluna de Particionamento**: Ao usar operações como `groupBy` ou `join`, você pode particionar os dados com base em uma coluna específica. Isso pode melhorar a eficiência do processamento, especialmente se os dados são frequentemente agrupados ou filtrados com base nessa coluna.\n",
        "\n",
        "   ```python\n",
        "   # Exemplo de particionamento por coluna\n",
        "   df = df.repartition('column_name')\n",
        "   ```\n",
        "\n",
        "### Impacto na Performance\n",
        "\n",
        "1. **Redução de Shuffle**: O particionamento adequado pode reduzir a quantidade de shuffle necessário. O shuffle ocorre quando os dados precisam ser redistribuídos entre diferentes partições e pode ser um processo caro em termos de tempo e recursos.\n",
        "\n",
        "2. **Balanceamento de Carga**: Se as partições forem muito grandes ou muito pequenas, isso pode causar um balanceamento desigual da carga de trabalho entre os nós do cluster. O particionamento adequado ajuda a garantir que cada nó esteja trabalhando com uma quantidade equilibrada de dados.\n",
        "\n",
        "3. **Melhoria da Localidade dos Dados**: Particionamento pode melhorar a localidade dos dados, ou seja, dados que são frequentemente acessados juntos são colocados na mesma partição. Isso reduz o tempo de acesso e melhora a eficiência.\n",
        "\n",
        "4. **Performance de Operações**: Operações como `join`, `groupBy`, e `aggregate` podem ser muito mais eficientes quando o particionamento é feito com base nas colunas relevantes. Isso minimiza a quantidade de dados que precisam ser movidos entre partições.\n",
        "\n",
        "5. **Efeito da Persistência**: Se você estiver persistindo dados em memória (`cache()` ou `persist()`), o particionamento adequado também pode impactar a eficiência do armazenamento e recuperação desses dados.\n",
        "\n",
        "### Boas Práticas para Particionamento\n",
        "\n",
        "1. **Ajustar o Número de Partições**: O número ideal de partições depende do tamanho dos dados e dos recursos disponíveis no cluster. Testar e ajustar o número de partições pode melhorar a performance.\n",
        "\n",
        "2. **Evitar Partições Pequenas**: Partições muito pequenas podem resultar em overhead excessivo. O ideal é ter um número razoável de partições, onde cada uma contém uma quantidade significativa de dados.\n",
        "\n",
        "3. **Escolher Colunas Adequadas para Particionamento**: Ao particionar com base em uma coluna, escolha uma coluna que distribua os dados de maneira uniforme e que seja frequentemente usada em operações de agrupamento ou junção.\n",
        "\n",
        "4. **Monitorar e Ajustar**: Utilize ferramentas de monitoramento do Spark para analisar a performance e ajustar o particionamento conforme necessário.\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "O particionamento é uma ferramenta poderosa no PySpark para otimizar o processamento de grandes volumes de dados. Um particionamento eficaz pode reduzir o tempo de processamento, melhorar o balanceamento de carga e minimizar o custo de operações de shuffle. Com uma compreensão adequada e a aplicação de boas práticas, você pode maximizar a eficiência das suas análises em big data.\n"
      ],
      "metadata": {
        "id": "vDBI64jw9W69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lkYnoQE79bcf",
        "outputId": "c0e4d05c-5878-4609-ff60-7b83dcaf1796"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas e os dados"
      ],
      "metadata": {
        "id": "fcl1att39k82"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "\n",
        "# Inicializando uma Spark Session\n",
        "spark = SparkSession.builder.appName(\"TitleBasicsExample\").getOrCreate()\n",
        "\n",
        "# Lendo o arquivo TSV comprimido\n",
        "df = spark.read.csv(\"title.basics.tsv.gz\", sep='\\t', header=True, inferSchema=True)\n",
        "\n",
        "# Exibindo o esquema do DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Exibindo as primeiras 5 linhas do DataFrame\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHpC8fEZ9p79",
        "outputId": "e4e79cf9-f5ad-429b-cf0f-dfdce233698e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- tconst: string (nullable = true)\n",
            " |-- titleType: string (nullable = true)\n",
            " |-- primaryTitle: string (nullable = true)\n",
            " |-- originalTitle: string (nullable = true)\n",
            " |-- isAdult: string (nullable = true)\n",
            " |-- startYear: string (nullable = true)\n",
            " |-- endYear: string (nullable = true)\n",
            " |-- runtimeMinutes: string (nullable = true)\n",
            " |-- genres: string (nullable = true)\n",
            "\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
            "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
            "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|     \\N|             1|   Documentary,Short|\n",
            "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|     Animation,Short|\n",
            "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|     \\N|             5|Action,Adventure,...|\n",
            "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|            12|     Animation,Short|\n",
            "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|     \\N|             1|        Comedy,Short|\n",
            "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_count = df.count()\n",
        "\n",
        "print(f\"Número total de registros: {total_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM9otqMV9rPV",
        "outputId": "59e38adf-4537-4c7d-ea34-e80b3262acc8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de registros: 8722919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Marcar o início do tempo\n",
        "start_time = time.time()\n",
        "\n",
        "# Realizar o groupBy e contar os registros\n",
        "df_grouped = df.groupBy(\"titleType\").count()\n",
        "\n",
        "# Coletar os resultados para visualizar (isto pode demorar dependendo do tamanho dos dados)\n",
        "result = df_grouped.collect()\n",
        "\n",
        "# Marcar o fim do tempo\n",
        "end_time = time.time()\n",
        "\n",
        "# Exibir o tempo decorrido\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tempo decorrido: {elapsed_time:.2f} segundos\")\n",
        "\n",
        "# Exibir os resultados do groupBy\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POOHRVEg9s4V",
        "outputId": "55740af6-179c-4cc6-e163-ba1fe84e2e23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo decorrido: 24.66 segundos\n",
            "+------------+-------+\n",
            "|   titleType|  count|\n",
            "+------------+-------+\n",
            "|    tvSeries| 215743|\n",
            "|tvMiniSeries|  43668|\n",
            "|     tvMovie| 123422|\n",
            "|     tvPilot|      1|\n",
            "|   tvEpisode|6642733|\n",
            "|       movie| 578512|\n",
            "|   tvSpecial|  42668|\n",
            "|       video| 242366|\n",
            "|   videoGame|  31505|\n",
            "|     tvShort|   6931|\n",
            "|       short| 795370|\n",
            "+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Particionando o DataFrame em 10 partições:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zSENXOXx9vY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_particionado_num = df.repartition(10)\n",
        "\n",
        "# Exibindo o número de partições\n",
        "print(\"Número de partições:\", df_particionado_num.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ADveDsl9yQV",
        "outputId": "0b92068d-807d-49c9-d164-4e77257b039f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de partições: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvando o DataFrame particionado em formato Parquet\n",
        "df_particionado_num.write.parquet(\"output/particionado_num\")"
      ],
      "metadata": {
        "id": "I7-OrdC490Ie"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lendo os dados particionados por número\n",
        "df_verificado_num = spark.read.parquet(\"output/particionado_num\")\n",
        "\n",
        "# Exibindo o esquema do DataFrame verificado\n",
        "df_verificado_num.printSchema()\n",
        "\n",
        "# Exibindo as primeiras 5 linhas do DataFrame verificado\n",
        "df_verificado_num.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA2WtyMr910m",
        "outputId": "224827eb-20c0-40e1-f4fe-92caaa28333a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- tconst: string (nullable = true)\n",
            " |-- titleType: string (nullable = true)\n",
            " |-- primaryTitle: string (nullable = true)\n",
            " |-- originalTitle: string (nullable = true)\n",
            " |-- isAdult: string (nullable = true)\n",
            " |-- startYear: string (nullable = true)\n",
            " |-- endYear: string (nullable = true)\n",
            " |-- runtimeMinutes: string (nullable = true)\n",
            " |-- genres: string (nullable = true)\n",
            "\n",
            "+----------+---------+---------------+---------------+-------+---------+-------+--------------+-------------+\n",
            "|    tconst|titleType|   primaryTitle|  originalTitle|isAdult|startYear|endYear|runtimeMinutes|       genres|\n",
            "+----------+---------+---------------+---------------+-------+---------+-------+--------------+-------------+\n",
            "|tt18927514|tvEpisode| Episode #1.198| Episode #1.198|      0|       \\N|     \\N|            \\N|        Drama|\n",
            "|tt12941206|tvEpisode|Episode #1.2790|Episode #1.2790|      0|     1984|     \\N|            43|Drama,Romance|\n",
            "|tt29386795|tvEpisode|  Episode #1.12|  Episode #1.12|      0|     2023|     \\N|            \\N|           \\N|\n",
            "|tt21859312|tvEpisode|  Episode #1.15|  Episode #1.15|      0|     2018|     \\N|            \\N|        Drama|\n",
            "|tt32510234|tvEpisode|  Episode #6.83|  Episode #6.83|      0|     1999|     \\N|            \\N|    Game-Show|\n",
            "+----------+---------+---------------+---------------+-------+---------+-------+--------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_in_partition(index, iterator):\n",
        "    count = sum(1 for _ in iterator)\n",
        "    yield (index, count)\n",
        "\n",
        "# Aplicando a função para contar linhas em cada partição\n",
        "partition_counts = df_particionado_num.rdd.mapPartitionsWithIndex(count_in_partition).collect()\n",
        "\n",
        "# Exibindo o número de linhas em cada partição\n",
        "for partition, count in partition_counts:\n",
        "    print(f\"Partição {partition}: {count} linhas\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN0AU8Tj935F",
        "outputId": "5f2b764f-88c0-4d97-ef8b-e8758553b27e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partição 0: 872292 linhas\n",
            "Partição 1: 872292 linhas\n",
            "Partição 2: 872292 linhas\n",
            "Partição 3: 872292 linhas\n",
            "Partição 4: 872292 linhas\n",
            "Partição 5: 872292 linhas\n",
            "Partição 6: 872292 linhas\n",
            "Partição 7: 872292 linhas\n",
            "Partição 8: 872291 linhas\n",
            "Partição 9: 872292 linhas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Marcar o início do tempo\n",
        "start_time = time.time()\n",
        "\n",
        "# Realizar o groupBy e contar os registros\n",
        "df_grouped = df_particionado_num.groupBy(\"titleType\").count()\n",
        "\n",
        "# Coletar os resultados para visualizar (isto pode demorar dependendo do tamanho dos dados)\n",
        "result = df_grouped.collect()\n",
        "\n",
        "# Marcar o fim do tempo\n",
        "end_time = time.time()\n",
        "\n",
        "# Exibir o tempo decorrido\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tempo decorrido: {elapsed_time:.2f} segundos\")\n",
        "\n",
        "# Exibir os resultados do groupBy\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYgymHYy96dG",
        "outputId": "09897466-a08d-4537-9301-e100384a3d0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo decorrido: 37.71 segundos\n",
            "+------------+-------+\n",
            "|   titleType|  count|\n",
            "+------------+-------+\n",
            "|    tvSeries| 215743|\n",
            "|tvMiniSeries|  43668|\n",
            "|     tvMovie| 123422|\n",
            "|   tvEpisode|6642733|\n",
            "|       movie| 578512|\n",
            "|   tvSpecial|  42668|\n",
            "|       video| 242366|\n",
            "|   videoGame|  31505|\n",
            "|     tvShort|   6931|\n",
            "|       short| 795370|\n",
            "|     tvPilot|      1|\n",
            "+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Particionamento por coluna"
      ],
      "metadata": {
        "id": "VLbhbKon99zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "df_filtered = df.filter(df.titleType == 'tvEpisode')\n",
        "start_time = time.time()\n",
        "\n",
        "# Realizar o groupBy por startYear e contar os registros\n",
        "df_grouped = df_filtered.groupBy(\"startYear\").count()\n",
        "\n",
        "# Coletar os resultados para visualizar (isto pode demorar dependendo do tamanho dos dados)\n",
        "result = df_grouped.collect()\n",
        "\n",
        "# Marcar o fim do tempo\n",
        "end_time = time.time()\n",
        "\n",
        "# Exibir o tempo decorrido\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tempo decorrido: {elapsed_time:.2f} segundos\")\n",
        "\n",
        "# Exibir os resultados do groupBy\n",
        "df_grouped.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGjFjDUN-BMP",
        "outputId": "3bd14f18-9f3c-4801-b138-041bf575fb74"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo decorrido: 23.01 segundos\n",
            "+---------+------+\n",
            "|startYear| count|\n",
            "+---------+------+\n",
            "|     1953|  6512|\n",
            "|     1957| 10186|\n",
            "|     1987| 27817|\n",
            "|     1956|  8511|\n",
            "|     2016| 99645|\n",
            "|     1936|    46|\n",
            "|     2012|201333|\n",
            "|     2020|323994|\n",
            "|     1958| 11140|\n",
            "|     1943|    25|\n",
            "|     1972| 19811|\n",
            "|     1931|    14|\n",
            "|     2026|    94|\n",
            "|     1988| 26860|\n",
            "|     1938|    57|\n",
            "|     2019|301583|\n",
            "|     2017|113702|\n",
            "|     1932|     3|\n",
            "|     1977| 21853|\n",
            "|     1971| 21005|\n",
            "+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Particionando o DataFrame com base na coluna titleType\n",
        "df_particionado_col = df.repartition(\"titleType\")\n",
        "\n",
        "# Salvando o DataFrame particionado em formato Parquet\n",
        "df_particionado_col.write.partitionBy(\"titleType\").parquet(\"output/particionado_col\")"
      ],
      "metadata": {
        "id": "bTT5nuVI-Cy9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Particionando o DataFrame com base na coluna titleType\n",
        "df_particionado_col = df.repartition(\"titleType\")\n",
        "\n",
        "# Verificando o número de partições\n",
        "print(\"Número de partições:\", df_particionado_col.rdd.getNumPartitions())\n",
        "\n",
        "# Contando o número de registros em cada partição\n",
        "df_particionado_col.groupBy(\"titleType\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJxXEhKr-Esd",
        "outputId": "ee5d53de-0df4-4e61-f6b3-873bbb878849"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de partições: 5\n",
            "+------------+-------+\n",
            "|   titleType|  count|\n",
            "+------------+-------+\n",
            "|    tvSeries| 215743|\n",
            "|tvMiniSeries|  43668|\n",
            "|     tvMovie| 123422|\n",
            "|     tvPilot|      1|\n",
            "|   tvEpisode|6642733|\n",
            "|       movie| 578512|\n",
            "|   tvSpecial|  42668|\n",
            "|       video| 242366|\n",
            "|   videoGame|  31505|\n",
            "|     tvShort|   6931|\n",
            "|       short| 795370|\n",
            "+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "df_filtered = df_particionado_col.filter(df.titleType == 'tvEpisode')\n",
        "\n",
        "df_grouped = df_filtered.groupBy(\"startYear\").count()\n",
        "\n",
        "# Coletar os resultados para visualizar (isto pode demorar dependendo do tamanho dos dados)\n",
        "result = df_grouped.collect()\n",
        "\n",
        "# Marcar o fim do tempo\n",
        "end_time = time.time()\n",
        "\n",
        "# Exibir o tempo decorrido\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tempo decorrido: {elapsed_time:.2f} segundos\")\n",
        "\n",
        "# Exibir os resultados do groupBy\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbuP8S4x-HNt",
        "outputId": "bdd50c93-c941-4f7f-b9e6-67abe1d113c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo decorrido: 25.66 segundos\n",
            "+---------+------+\n",
            "|startYear| count|\n",
            "+---------+------+\n",
            "|     1953|  6512|\n",
            "|     1957| 10186|\n",
            "|     1987| 27817|\n",
            "|     1956|  8511|\n",
            "|     2016| 99645|\n",
            "|     1936|    46|\n",
            "|     2012|201333|\n",
            "|     2020|323994|\n",
            "|     1958| 11140|\n",
            "|     1943|    25|\n",
            "|     1972| 19811|\n",
            "|     1931|    14|\n",
            "|     2026|    94|\n",
            "|     1988| 26860|\n",
            "|     1938|    57|\n",
            "|     2019|301583|\n",
            "|     2017|113702|\n",
            "|     1932|     3|\n",
            "|     1977| 21853|\n",
            "|     1971| 21005|\n",
            "+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}